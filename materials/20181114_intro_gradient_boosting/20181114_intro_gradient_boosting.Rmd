---
title: "Intro to Gradient Tree Boosting"
output:
  pdf_document:
    keep_tex: true
geometry: margin=1.5cm
header-includes:
   - \usepackage{booktabs}
   - \usepackage{multicol}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Introduction

### Goal

 * Ensemble model
 * Component models are diverse

### Previous Strategies

1. Pick models that are different from each other in some way:

    * different model structure
    * different training sets (bagging)
    * different use of features

2. Estimate the models totally separately from each other

3. Put them together by averaging, majority vote, or stacking


#### Specific example: random forests

 * Each tree used a different training set (bagged features)
 * Each tree uses a random subset of features in searching for each split.
 * The trees are all estimated separately, then predictions are averaged later.
 * For example, for regression:

$$\hat{f}^{(ensemble)}(x_i) = \frac{1}{B} \sum_b \hat{f}^{(b)}(x_i)$$

Here, $\hat{f}^{(b)}(x_i)$ represents the prediction from one tree in the forest;

$\hat{f}^{(ensemble)}(x_i)$ is the random forest prediction.

### New Strategy: Boosting

Boosting takes a sequential approach to estimation:

 1. Start with a simple initial model (e.g., for regression start by predicting the mean).
 2. Repeat the following:
    a. Fit a model that is specifically tuned to training set observations that the current ensemble does not predict well
    b. Update the ensemble by adding in this new model

Why is this a good idea?

 * Component models are specifically different from what's already in the ensemble!

\newpage

### A Specific Example: Gradient Tree Boosting

Let's start with building some intuition for the method, and define it more carefully later.

In this example, our component models will be "stumps": trees with only one split.

Here's a made up regression problem, and an initial prediction for each observation, given by the sample mean for the response variable.

```{r, echo=FALSE, message = FALSE, warning=FALSE}
library(purrr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(rpart)

set.seed(96442)

n <- 25
train_data <- data.frame(
  x = runif(n = n, min = -10, max = 10)
) %>%
  mutate(
    y = 50 + 2 * x - 12 * x + 0.2 * x^3 + rnorm(n, sd = 10)
  )

ggplot(data = train_data, mapping = aes(x = x, y = y)) +
  geom_point()
```

```{r, echo=FALSE}
calc_f_hat <- function(x, b0, tree_fits, tree_weights) {
  x_df <- data.frame(x = x)
  
  if(length(tree_fits) > 0) {
    tree_preds <- map_dfc(tree_fits, predict, newdata = x_df) %>%
      as.matrix()
    tree_preds <- tree_preds %*% tree_weights
    tree_preds <- tree_preds[, 1]
  } else {
    tree_preds <- rep(0, length(x))
  }
  
  results <- b0 + tree_preds
  
  return(results)
}
```

```{r, fig.height=6, echo = FALSE}
component_alpha <- 0.3
component_num <- 1

b0 <- mean(train_data$y)
tree_fits <- vector("list", 0)
tree_weights <- NULL

x_grid <- data.frame(
  x = seq(from = -10, to = 10, length = 501)
)

component_model_preds_grid <- x_grid %>%
  mutate(
    component_num = component_num,
    y_hat = b0
  )

plot_df_grid <- x_grid %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights)
  )

plot_df <- train_data %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights),
    derivative = y - y_hat # 2 * (y - y_hat)
  )

p_comp <- ggplot() +
  geom_line(data = component_model_preds_grid, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = 0.2, color = "red") +
  ggtitle("All Current Component Model Predictions")

p1 <- ggplot() +
  geom_errorbar(data = plot_df %>% filter(y < y_hat), mapping = aes(x = x, ymin = y, ymax = y_hat), color = "cornflowerblue") +
  geom_errorbar(data = plot_df %>% filter(y >= y_hat), mapping = aes(x = x, ymin = y_hat, ymax = y), color = "cornflowerblue") +
  geom_point(data = train_data, mapping = aes(x = x, y = y)) +
  geom_line(data = component_model_preds_grid, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = component_alpha, color = "red") +
  geom_line(data = plot_df_grid, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = plot_df, mapping = aes(x = x, y = y_hat), color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Ensemble Predictions")

p2 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("Response variable for next component model:\nWhere does current ensemble go wrong?")

#grid.arrange(p_comp, p1, p2, ncol = 1)
grid.arrange(p1, p2, ncol = 1)
```

\newpage

```{r, fig.height=9, echo=FALSE}
component_num <- component_num + 1

new_tree <- rpart(derivative ~ x, data = plot_df,
  control = rpart.control(
    maxdepth = 1,
    cp = 0,
    maxsurrogate = 0,
    maxcompete = 0,
    minsplit = 2,
    minbucket = 1)
  )
tree_fits <- c(tree_fits, list(new_tree))
tree_weights <- c(tree_weights, 1)

new_tree_preds_grid_df <- x_grid %>%
  mutate(
    y_hat = predict(new_tree, newdata = x_grid),
    component_num = component_num
  )

new_tree_preds_grid_df_0.01 <- new_tree_preds_grid_df %>%
  mutate(
    y_hat = 0.1 * y_hat
  )

new_tree_preds_df <- train_data %>%
  mutate(
    y_hat = predict(new_tree, newdata = train_data),
    component_num = component_num
  )

new_tree_preds_df_0.01 <- new_tree_preds_df %>%
  mutate(
    y_hat = 0.1 * y_hat
  )

component_model_preds_grid_0.01 <- bind_rows(
  component_model_preds_grid,
  new_tree_preds_grid_df_0.01
)

component_model_preds_grid <- bind_rows(
  component_model_preds_grid,
  new_tree_preds_grid_df
)

p0 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_line(data = new_tree_preds_grid_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = new_tree_preds_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("New component model fit")

p0_0.01 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_line(data = new_tree_preds_grid_df_0.01, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = new_tree_preds_df_0.01, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("New component model fit; downweighted")

plot_df_grid <- x_grid %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights)
  )

plot_df <- train_data %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights),
    derivative = y - y_hat #2 * (y - y_hat)
  )


plot_df_grid_0.01 <- x_grid %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = 0.1)
  )

plot_df_0.01 <- train_data %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = 0.1),
    derivative = y - y_hat #2 * (y - y_hat)
  )

p_comp <- ggplot() +
  geom_line(data = component_model_preds_grid, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = component_alpha, color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Component Model Predictions")

p_comp_0.01 <- ggplot() +
  geom_line(data = component_model_preds_grid_0.01, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = component_alpha, color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Component Model Predictions; downweighted")


p1 <- ggplot() +
  geom_errorbar(data = plot_df %>% filter(y < y_hat),
    mapping = aes(x = x, ymin = y, ymax = y_hat),
    color = "cornflowerblue") +
  geom_errorbar(data = plot_df %>% filter(y >= y_hat),
    mapping = aes(x = x, ymin = y_hat, ymax = y),
    color = "cornflowerblue") +
  geom_point(data = train_data, mapping = aes(x = x, y = y)) +
  geom_line(data = plot_df_grid, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = plot_df, mapping = aes(x = x, y = y_hat), color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Ensemble Predictions")

p1_0.01 <- ggplot() +
  geom_errorbar(data = plot_df_0.01 %>% filter(y < y_hat),
    mapping = aes(x = x, ymin = y, ymax = y_hat),
    color = "cornflowerblue") +
  geom_errorbar(data = plot_df_0.01 %>% filter(y >= y_hat),
    mapping = aes(x = x, ymin = y_hat, ymax = y),
    color = "cornflowerblue") +
  geom_point(data = train_data, mapping = aes(x = x, y = y)) +
  geom_line(data = plot_df_grid_0.01, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = plot_df_0.01, mapping = aes(x = x, y = y_hat), color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Ensemble Predictions; downweighted")

p2 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("Response variable for next component model:\nWhere does current ensemble go wrong?")

grid.arrange(p0, p_comp, p1, p2, ncol = 1)


p0_step1 <- p0
p_comp_step1 <- p_comp
p1_step1 <- p1
```


\newpage

```{r, fig.height=9, echo=FALSE}
component_num <- component_num + 1

new_tree <- rpart(derivative ~ x, data = plot_df,
  control = rpart.control(
    maxdepth = 1,
    cp = 0,
    maxsurrogate = 0,
    maxcompete = 0,
    minsplit = 2,
    minbucket = 1)
  )
tree_fits <- c(tree_fits, list(new_tree))
tree_weights <- c(tree_weights, 1)

new_tree_preds_grid_df <- x_grid %>%
  mutate(
    y_hat = predict(new_tree, newdata = x_grid),
    component_num = component_num
  )

new_tree_preds_df <- train_data %>%
  mutate(
    y_hat = predict(new_tree, newdata = train_data),
    component_num = component_num
  )

component_model_preds_grid <- bind_rows(
  component_model_preds_grid,
  new_tree_preds_grid_df
)

p0 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_line(data = new_tree_preds_grid_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = new_tree_preds_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("New component model fit")

plot_df_grid <- x_grid %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights)
  )

plot_df <- train_data %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights),
    derivative = y - y_hat #2 * (y - y_hat)
  )

p_comp <- ggplot() +
  geom_line(data = component_model_preds_grid, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = component_alpha, color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Component Model Predictions")


p1 <- ggplot() +
  geom_errorbar(data = plot_df %>% filter(y < y_hat),
    mapping = aes(x = x, ymin = y, ymax = y_hat),
    color = "cornflowerblue") +
  geom_errorbar(data = plot_df %>% filter(y >= y_hat),
    mapping = aes(x = x, ymin = y_hat, ymax = y),
    color = "cornflowerblue") +
  geom_point(data = train_data, mapping = aes(x = x, y = y)) +
  geom_line(data = plot_df_grid, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = plot_df, mapping = aes(x = x, y = y_hat), color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Ensemble Predictions")

p2 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("Response variable for next component model:\nWhere does current ensemble go wrong?")

grid.arrange(p0, p_comp, p1, p2, ncol = 1)
```


\newpage

```{r, fig.height=9, echo=FALSE}
component_num <- component_num + 1

new_tree <- rpart(derivative ~ x, data = plot_df,
  control = rpart.control(
    maxdepth = 1,
    cp = 0,
    maxsurrogate = 0,
    maxcompete = 0,
    minsplit = 2,
    minbucket = 1)
  )
tree_fits <- c(tree_fits, list(new_tree))
tree_weights <- c(tree_weights, 1)

new_tree_preds_grid_df <- x_grid %>%
  mutate(
    y_hat = predict(new_tree, newdata = x_grid),
    component_num = component_num
  )

new_tree_preds_df <- train_data %>%
  mutate(
    y_hat = predict(new_tree, newdata = train_data),
    component_num = component_num
  )

component_model_preds_grid <- bind_rows(
  component_model_preds_grid,
  new_tree_preds_grid_df
)

p0 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_line(data = new_tree_preds_grid_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = new_tree_preds_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("New component model fit")

plot_df_grid <- x_grid %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights)
  )

plot_df <- train_data %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights),
    derivative = y - y_hat #2 * (y - y_hat)
  )

p_comp <- ggplot() +
  geom_line(data = component_model_preds_grid, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = component_alpha, color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Component Model Predictions")


p1 <- ggplot() +
  geom_errorbar(data = plot_df %>% filter(y < y_hat),
    mapping = aes(x = x, ymin = y, ymax = y_hat),
    color = "cornflowerblue") +
  geom_errorbar(data = plot_df %>% filter(y >= y_hat),
    mapping = aes(x = x, ymin = y_hat, ymax = y),
    color = "cornflowerblue") +
  geom_point(data = train_data, mapping = aes(x = x, y = y)) +
  geom_line(data = plot_df_grid, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = plot_df, mapping = aes(x = x, y = y_hat), color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Ensemble Predictions")

p2 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("Response variable for next component model:\nWhere does current ensemble go wrong?")

grid.arrange(p0, p_comp, p1, p2, ncol = 1)
```


\newpage

```{r, fig.height=9, echo=FALSE}
component_num <- component_num + 1

new_tree <- rpart(derivative ~ x, data = plot_df,
  control = rpart.control(
    maxdepth = 1,
    cp = 0,
    maxsurrogate = 0,
    maxcompete = 0,
    minsplit = 2,
    minbucket = 1)
  )
tree_fits <- c(tree_fits, list(new_tree))
tree_weights <- c(tree_weights, 1)

new_tree_preds_grid_df <- x_grid %>%
  mutate(
    y_hat = predict(new_tree, newdata = x_grid),
    component_num = component_num
  )

new_tree_preds_df <- train_data %>%
  mutate(
    y_hat = predict(new_tree, newdata = train_data),
    component_num = component_num
  )

component_model_preds_grid <- bind_rows(
  component_model_preds_grid,
  new_tree_preds_grid_df
)

p0 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_line(data = new_tree_preds_grid_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = new_tree_preds_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("New component model fit")

plot_df_grid <- x_grid %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights)
  )

plot_df <- train_data %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights),
    derivative = y - y_hat #2 * (y - y_hat)
  )

p_comp <- ggplot() +
  geom_line(data = component_model_preds_grid, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = component_alpha, color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Component Model Predictions")


p1 <- ggplot() +
  geom_errorbar(data = plot_df %>% filter(y < y_hat),
    mapping = aes(x = x, ymin = y, ymax = y_hat),
    color = "cornflowerblue") +
  geom_errorbar(data = plot_df %>% filter(y >= y_hat),
    mapping = aes(x = x, ymin = y_hat, ymax = y),
    color = "cornflowerblue") +
  geom_point(data = train_data, mapping = aes(x = x, y = y)) +
  geom_line(data = plot_df_grid, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = plot_df, mapping = aes(x = x, y = y_hat), color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Ensemble Predictions")

p2 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("Response variable for next component model:\nWhere does current ensemble go wrong?")

grid.arrange(p0, p_comp, p1, p2, ncol = 1)
```


\newpage

```{r, fig.height=9, echo=FALSE}
component_num <- component_num + 1

new_tree <- rpart(derivative ~ x, data = plot_df,
  control = rpart.control(
    maxdepth = 1,
    cp = 0,
    maxsurrogate = 0,
    maxcompete = 0,
    minsplit = 2,
    minbucket = 1)
  )
tree_fits <- c(tree_fits, list(new_tree))
tree_weights <- c(tree_weights, 1)

new_tree_preds_grid_df <- x_grid %>%
  mutate(
    y_hat = predict(new_tree, newdata = x_grid),
    component_num = component_num
  )

new_tree_preds_df <- train_data %>%
  mutate(
    y_hat = predict(new_tree, newdata = train_data),
    component_num = component_num
  )

component_model_preds_grid <- bind_rows(
  component_model_preds_grid,
  new_tree_preds_grid_df
)

p0 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_line(data = new_tree_preds_grid_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = new_tree_preds_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("New component model fit")

plot_df_grid <- x_grid %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights)
  )

plot_df <- train_data %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights),
    derivative = y - y_hat #2 * (y - y_hat)
  )

p_comp <- ggplot() +
  geom_line(data = component_model_preds_grid, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = component_alpha, color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Component Model Predictions")


p1 <- ggplot() +
  geom_errorbar(data = plot_df %>% filter(y < y_hat),
    mapping = aes(x = x, ymin = y, ymax = y_hat),
    color = "cornflowerblue") +
  geom_errorbar(data = plot_df %>% filter(y >= y_hat),
    mapping = aes(x = x, ymin = y_hat, ymax = y),
    color = "cornflowerblue") +
  geom_point(data = train_data, mapping = aes(x = x, y = y)) +
  geom_line(data = plot_df_grid, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = plot_df, mapping = aes(x = x, y = y_hat), color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Ensemble Predictions")

p2 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("Response variable for next component model:\nWhere does current ensemble go wrong?")

grid.arrange(p0, p_comp, p1, p2, ncol = 1)
```

\newpage

```{r, fig.height=9, echo=FALSE}
component_num <- component_num + 1

new_tree <- rpart(derivative ~ x, data = plot_df,
  control = rpart.control(
    maxdepth = 1,
    cp = 0,
    maxsurrogate = 0,
    maxcompete = 0,
    minsplit = 2,
    minbucket = 1)
  )
tree_fits <- c(tree_fits, list(new_tree))
tree_weights <- c(tree_weights, 1)

new_tree_preds_grid_df <- x_grid %>%
  mutate(
    y_hat = predict(new_tree, newdata = x_grid),
    component_num = component_num
  )

new_tree_preds_df <- train_data %>%
  mutate(
    y_hat = predict(new_tree, newdata = train_data),
    component_num = component_num
  )

component_model_preds_grid <- bind_rows(
  component_model_preds_grid,
  new_tree_preds_grid_df
)

p0 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_line(data = new_tree_preds_grid_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = new_tree_preds_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("New component model fit")

plot_df_grid <- x_grid %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights)
  )

plot_df <- train_data %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights),
    derivative = y - y_hat #2 * (y - y_hat)
  )

p_comp <- ggplot() +
  geom_line(data = component_model_preds_grid, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = component_alpha, color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Component Model Predictions")


p1 <- ggplot() +
  geom_errorbar(data = plot_df %>% filter(y < y_hat),
    mapping = aes(x = x, ymin = y, ymax = y_hat),
    color = "cornflowerblue") +
  geom_errorbar(data = plot_df %>% filter(y >= y_hat),
    mapping = aes(x = x, ymin = y_hat, ymax = y),
    color = "cornflowerblue") +
  geom_point(data = train_data, mapping = aes(x = x, y = y)) +
  geom_line(data = plot_df_grid, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = plot_df, mapping = aes(x = x, y = y_hat), color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Ensemble Predictions")

p2 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("Response variable for next component model:\nWhere does current ensemble go wrong?")

grid.arrange(p0, p_comp, p1, p2, ncol = 1)
```


\newpage

```{r, fig.height=9, echo=FALSE}
component_num <- component_num + 1

new_tree <- rpart(derivative ~ x, data = plot_df,
  control = rpart.control(
    maxdepth = 1,
    cp = 0,
    maxsurrogate = 0,
    maxcompete = 0,
    minsplit = 2,
    minbucket = 1)
  )
tree_fits <- c(tree_fits, list(new_tree))
tree_weights <- c(tree_weights, 1)

new_tree_preds_grid_df <- x_grid %>%
  mutate(
    y_hat = predict(new_tree, newdata = x_grid),
    component_num = component_num
  )

new_tree_preds_df <- train_data %>%
  mutate(
    y_hat = predict(new_tree, newdata = train_data),
    component_num = component_num
  )

component_model_preds_grid <- bind_rows(
  component_model_preds_grid,
  new_tree_preds_grid_df
)

p0 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_line(data = new_tree_preds_grid_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = new_tree_preds_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ggtitle("New component model fit")

plot_df_grid <- x_grid %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights)
  )

plot_df <- train_data %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights),
    derivative = y - y_hat #2 * (y - y_hat)
  )

p_comp <- ggplot() +
  geom_line(data = component_model_preds_grid, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = component_alpha, color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Component Model Predictions")


p1 <- ggplot() +
  geom_errorbar(data = plot_df %>% filter(y < y_hat),
    mapping = aes(x = x, ymin = y, ymax = y_hat),
    color = "cornflowerblue") +
  geom_errorbar(data = plot_df %>% filter(y >= y_hat),
    mapping = aes(x = x, ymin = y_hat, ymax = y),
    color = "cornflowerblue") +
  geom_point(data = train_data, mapping = aes(x = x, y = y)) +
  geom_line(data = plot_df_grid, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = plot_df, mapping = aes(x = x, y = y_hat), color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Ensemble Predictions")

p2 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("Response variable for next component model:\nWhere does current ensemble go wrong?")

grid.arrange(p0, p_comp, p1, p2, ncol = 1)
```


\newpage

```{r, fig.height=9, echo=FALSE}
component_num <- component_num + 1

new_tree <- rpart(derivative ~ x, data = plot_df,
  control = rpart.control(
    maxdepth = 1,
    cp = 0,
    maxsurrogate = 0,
    maxcompete = 0,
    minsplit = 2,
    minbucket = 1)
  )
tree_fits <- c(tree_fits, list(new_tree))
tree_weights <- c(tree_weights, 1)

new_tree_preds_grid_df <- x_grid %>%
  mutate(
    y_hat = predict(new_tree, newdata = x_grid),
    component_num = component_num
  )

new_tree_preds_df <- train_data %>%
  mutate(
    y_hat = predict(new_tree, newdata = train_data),
    component_num = component_num
  )

component_model_preds_grid <- bind_rows(
  component_model_preds_grid,
  new_tree_preds_grid_df
)

p0 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_line(data = new_tree_preds_grid_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = new_tree_preds_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ggtitle("New component model fit")

plot_df_grid <- x_grid %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights)
  )

plot_df <- train_data %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights),
    derivative = y - y_hat #2 * (y - y_hat)
  )

p_comp <- ggplot() +
  geom_line(data = component_model_preds_grid, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = component_alpha, color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Component Model Predictions")


p1 <- ggplot() +
  geom_errorbar(data = plot_df %>% filter(y < y_hat),
    mapping = aes(x = x, ymin = y, ymax = y_hat),
    color = "cornflowerblue") +
  geom_errorbar(data = plot_df %>% filter(y >= y_hat),
    mapping = aes(x = x, ymin = y_hat, ymax = y),
    color = "cornflowerblue") +
  geom_point(data = train_data, mapping = aes(x = x, y = y)) +
  geom_line(data = plot_df_grid, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = plot_df, mapping = aes(x = x, y = y_hat), color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Ensemble Predictions")

p2 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("Response variable for next component model:\nWhere does current ensemble go wrong?")

grid.arrange(p0, p_comp, p1, p2, ncol = 1)
```


\newpage

```{r, fig.height=9, echo=FALSE}
component_num <- component_num + 1

new_tree <- rpart(derivative ~ x, data = plot_df,
  control = rpart.control(
    maxdepth = 1,
    cp = 0,
    maxsurrogate = 0,
    maxcompete = 0,
    minsplit = 2,
    minbucket = 1)
  )
tree_fits <- c(tree_fits, list(new_tree))
tree_weights <- c(tree_weights, 1)

new_tree_preds_grid_df <- x_grid %>%
  mutate(
    y_hat = predict(new_tree, newdata = x_grid),
    component_num = component_num
  )

new_tree_preds_df <- train_data %>%
  mutate(
    y_hat = predict(new_tree, newdata = train_data),
    component_num = component_num
  )

component_model_preds_grid <- bind_rows(
  component_model_preds_grid,
  new_tree_preds_grid_df
)

p0 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_line(data = new_tree_preds_grid_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = new_tree_preds_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ggtitle("New component model fit")

plot_df_grid <- x_grid %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights)
  )

plot_df <- train_data %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights),
    derivative = y - y_hat #2 * (y - y_hat)
  )

p_comp <- ggplot() +
  geom_line(data = component_model_preds_grid, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = component_alpha, color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Component Model Predictions")


p1 <- ggplot() +
  geom_errorbar(data = plot_df %>% filter(y < y_hat),
    mapping = aes(x = x, ymin = y, ymax = y_hat),
    color = "cornflowerblue") +
  geom_errorbar(data = plot_df %>% filter(y >= y_hat),
    mapping = aes(x = x, ymin = y_hat, ymax = y),
    color = "cornflowerblue") +
  geom_point(data = train_data, mapping = aes(x = x, y = y)) +
  geom_line(data = plot_df_grid, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = plot_df, mapping = aes(x = x, y = y_hat), color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Ensemble Predictions")

p2 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("Response variable for next component model:\nWhere does current ensemble go wrong?")

grid.arrange(p0, p_comp, p1, p2, ncol = 1)
```


\newpage

```{r, fig.height=9, echo=FALSE}
component_num <- component_num + 1

new_tree <- rpart(derivative ~ x, data = plot_df,
  control = rpart.control(
    maxdepth = 1,
    cp = 0,
    maxsurrogate = 0,
    maxcompete = 0,
    minsplit = 2,
    minbucket = 1)
  )
tree_fits <- c(tree_fits, list(new_tree))
tree_weights <- c(tree_weights, 1)

new_tree_preds_grid_df <- x_grid %>%
  mutate(
    y_hat = predict(new_tree, newdata = x_grid),
    component_num = component_num
  )

new_tree_preds_df <- train_data %>%
  mutate(
    y_hat = predict(new_tree, newdata = train_data),
    component_num = component_num
  )

component_model_preds_grid <- bind_rows(
  component_model_preds_grid,
  new_tree_preds_grid_df
)

p0 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_line(data = new_tree_preds_grid_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = new_tree_preds_df, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("New component model fit")

plot_df_grid <- x_grid %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights)
  )

plot_df <- train_data %>%
  mutate(
    y_hat = calc_f_hat(x, b0 = b0, tree_fits = tree_fits, tree_weights = tree_weights),
    derivative = y - y_hat #2 * (y - y_hat)
  )

p_comp <- ggplot() +
  geom_line(data = component_model_preds_grid, mapping = aes(x = x, y = y_hat, group = factor(component_num)), alpha = component_alpha, color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Component Model Predictions")


p1 <- ggplot() +
  geom_errorbar(data = plot_df %>% filter(y < y_hat),
    mapping = aes(x = x, ymin = y, ymax = y_hat),
    color = "cornflowerblue") +
  geom_errorbar(data = plot_df %>% filter(y >= y_hat),
    mapping = aes(x = x, ymin = y_hat, ymax = y),
    color = "cornflowerblue") +
  geom_point(data = train_data, mapping = aes(x = x, y = y)) +
  geom_line(data = plot_df_grid, mapping = aes(x = x, y = y_hat), color = "red") +
  geom_point(data = plot_df, mapping = aes(x = x, y = y_hat), color = "red") +
  xlim(c(-10, 10)) +
  ggtitle("Current Ensemble Predictions")

p2 <- ggplot() +
  geom_point(data = plot_df, mapping = aes(x = x, y = derivative), color = "cornflowerblue") +
  geom_hline(yintercept = 0) +
  xlim(c(-10, 10)) +
  ylab("residual") +
  ggtitle("Response variable for next component model:\nWhere does current ensemble go wrong?")

grid.arrange(p0, p_comp, p1, p2, ncol = 1)
```

\newpage

### Mathematical Details: Gradient Boosting for Regression

In step $b$ of the gradient boosting process, we have a current "working" ensemble that gives predictions $\hat{y}_i = \hat{f}^{(ensemble, b)}(x_i)$.

Above, we motivated our procedure by fitting the next component model to the residuals from the current working ensemble.  Let's see how we could arrive at that procedure from a more general approach (that will also be useful for classification problems).

#### Set up in terms of optimizing RSS

 * We want to *minimize* the residual sum of squares
$$RSS = \sum_{i = 1}^n (y_i - \hat{y}_i)^2$$
 * ...but for our purpose now it's easier to think about *maximizing* the negative of the residual sum of squares:
$$-RSS = - \sum_{i = 1}^n (y_i - \hat{y}_i)^2$$
    * If a particular function $\hat{f}(x_i)$ gives predictions that minimize RSS on the training set, then
    * $\hat{f}(x_i)$ also maximizes -RSS on the training set


 * For reasons of emotional convenience (making things look familiar for our first pass), let's instead maximize $-\frac{1}{2} RSS$
$$-\frac{1}{2} RSS = - \frac{1}{2} \sum_{i = 1}^n (y_i - \hat{y}_i)^2$$
    * If a particular function $\hat{f}(x_i)$ gives predictions that maximize $-RSS$ on the training set, then
    * $\hat{f}(x_i)$ also maximizes $-\frac{1}{2}RSS$ on the training set

#### Derivatives of -RSS tell us how to change our current predictions

 * Each step in the gradient boosting process makes a small change to the predicted value $\hat{y}_i$ for each observation.
 * How should we change the predicted values?
 * The derivative of the negative RSS with respect to $\hat{y}_{i^*}$ (for a particular observation index $i^*$) tells us the rate of change of $-RSS$ if we make a small change to the predicted value $\hat{y}_{i^*}$ for that observation.

\begin{align*}
\frac{\partial}{\partial \hat{y}_{i^*}} -\frac{1}{2}RSS &= \frac{\partial}{\partial \hat{y}_{i^*}} \left[ -\frac{1}{2} \sum_{i = 1}^n (y_i - \hat{y}_i)^2 \right] \\
&= \frac{\partial}{\partial \hat{y}_{i^*}} -\frac{1}{2} \left[ (y_1 - \hat{y}_1)^2 + \cdots + (y_{i^*} - \hat{y}_{i^*})^2 + \cdots + (y_n - \hat{y}_n)^2 \right] \\
&= -\frac{1}{2} 2 (y_{i^*} - \hat{y}_{i^*}) (-1) \\
&= (y_{i^*} - \hat{y}_{i^*})
\end{align*}

Interpretation: The (partial) derivative is equal to the residual.

 * Suppose our current prediction is too small:
    * $y_{i^*} > \hat{y}_{i^*}$
    * $y_{i^*} - \hat{y}_{i^*} > 0$
    * Derivative is positive: We can increase -RSS by increasing $\hat{y}_{i^*}$
    * Larger difference between observed and predicted means larger derivative.
 * Suppose our current prediction is too large:
    * $y_{i^*} < \hat{y}_{i^*}$
    * $y_{i^*} - \hat{y}_{i^*} < 0$
    * Derivative is negative: We can increase -RSS by decreasing $\hat{y}_{i^*}$
    * Larger difference between observed and predicted means larger (magnitude) derivative.

#### Estimation of the next component model

1. Calculate the **gradient vector** of our objective function with respect to the predicted values, evaluated at the current predictions from our ensemble:

$$\nabla_{\hat{y}} \left( - \frac{1}{2} RSS \right) = -\frac{1}{2} \left(\frac{\partial}{\partial \hat{y}_{1}} RSS, \ldots, \frac{\partial}{\partial \hat{y}_{n}} RSS \right) = (r_1, \ldots, r_n)$$

2. Fit a component model using the vector $(r_1, \ldots, r_n)$ as the response

3. Add the new component model to the ensemble.

#### Opportunities for Regularization/Preventing Overfitting

 * **Learning Rate:** Multiply predictions from our new component model by a small weight like 0.01.  Prevents us from immediately overfitting training data.  Comparing step 1 with learning rate 1 and learning rate 0.1:

```{r, echo = FALSE}
grid.arrange(p0_step1, p0_0.01, p_comp_step1, p_comp_0.01, p1_step1, p1_0.01, ncol = 2)
```

 * **Number of boosting iterations:** The more boosting iterations we run, the more we run the risk of overfitting.

 * **Minimum Reduction in RSS:** How big does a gain from a split need to be, in order to make that split?

 * **Tree Depth:** Above we fit "decision stumps": 1 split only.  Allowing deeper trees allows more flexible models.
 
 * **Train on Fewer Observations:** Each tree trained using a subset of training set observations.
 
 * **Train on Fewer Features:** Each component model trained using a subset of available explanatory variables.

#### Miscellaneous Notes:

 * The scaling factor of $1/2$ is arbitrary - especially if we use a small learning rate.
 
 * At some point we will need to deal with a negative sign: either to get an objective function to maximize, or to indicate the direction to move to minimize RSS.
 
### Estimation with xgboost ("eXtreme Gradient Boosting")

 * Data scientists have gotten better at catchy names since the days of Type I/Type II errors.
 * One of several commonly used implementations of gradient boosting.  Written in C, interfaces to other languages like python
 * Second-most-commonly used option is lightgbm
 * Estimation can be done via the train function in the caret package.

Let's look at our favorite lidar data set:

```{r, fig.height=3}
library(readr)
lidar <- read_table2("http://www.evanlray.com/data/all-of-nonparametric-stat/lidar.dat")

tt_split <- caret::createDataPartition(lidar$logratio, p = 0.8)
lidar_train <- lidar %>% slice(tt_split[[1]])
lidar_test <- lidar %>% slice(-tt_split[[1]])

ggplot(data = lidar_train, mapping = aes(x = range, y = logratio)) +
  geom_point()
```

```{r}
library(caret)
xgb_fit <- train(
  logratio ~ range,
  data = lidar_train,
  method = "xgbTree",
  trControl = trainControl(method = "cv", number = 10, returnResamp = "all"),
  tuneGrid = expand.grid(
    nrounds = c(10, 50, 100),
    eta = 0.3, # learning rate; 0.3 is the default
    gamma = 0, # minimum loss reduction to make a split; 0 is the default
    max_depth = 1:5, # how deep are our trees?
    subsample = c(0.8, 1), # proportion of observations to use in growing each tree
    colsample_bytree = 1, # proportion of explanatory variables used in each tree
    min_child_weight = 1 # think of this as how many observations must be in each leaf node
  )
)

xgb_fit$results %>% select(nrounds, max_depth, subsample, RMSE)
```

Looks like we may be overfitting; our best RMSE is with the lowest values of max depth and nrounds.  Let's try a lower learning rate.  Also, subsample wasn't helpful.  Let's just stick with subsample = 1.

```{r}
library(caret)
xgb_fit <- train(
  logratio ~ range,
  data = lidar_train,
  method = "xgbTree",
  trControl = trainControl(method = "cv", number = 10, returnResamp = "all"),
  tuneGrid = expand.grid(
    nrounds = c(5, 10, 20, 30, 40),
    eta = c(0.1, 0.2, 0.3), # learning rate; 0.3 is the default
    gamma = 0, # minimum loss reduction to make a split; 0 is the default
    max_depth = 1:2, # how deep are our trees?
    subsample = 1, # proportion of observations to use in growing each tree
    colsample_bytree = 1, # proportion of explanatory variables used in each tree
    min_child_weight = 1 # think of this as how many observations must be in each leaf node
  )
)

xgb_fit$results %>% filter(RMSE == min(RMSE))
```

The best tuning parameter values were the middle of the ranges of values we tried (or at the edge of possible values, in the case of max\_depth); seems OK.

Let's look at the predictions:

```{r}
lidar_test <- lidar_test %>%
  mutate(
    logratio_hat = predict(xgb_fit, lidar_test)
  )

ggplot() +
  geom_point(data = lidar_train, mapping = aes(x = range, y = logratio)) +
  geom_point(data = lidar_test, mapping = aes(x = range, y = logratio), color = "orange") +
  geom_line(data = lidar_test, mapping = aes(x = range, y = logratio_hat), color = "orange")
```
